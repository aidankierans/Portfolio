# Portfolio
A selection of the classwork and extracurricular projects I'm most proud of. Currently biased towards projects completed recently.

# Main files
Efficient_Critical_Definitions.sage - I wrote this code in Sage in May of 2019, and optimized it over the course of the graph theory research project so that we could efficiently iterate through large sets of graphs and find the ones we cared about. 
Post Mortem: It was my first time using Python and Sage, as well as gt.sage (a library of graph theory objects and functions that my program assumes is already loaded). If I were to write it again today, I'd look into Cythonizing it to speed it up further, although it served its purpose as-is well enough at the time.
Depending on whether it's open source, I'm also planning on adding gt.sage to a folder with ECD (so that others can see ECD in action if they want to).

CSV Data Imputation/a2.py - I wrote this code in Python in September of 2019 for CMSC 435: Intro to Data Science. The purpose of the program is to take three csv files—one missing 5% of the values, one missing 20% of the values, and one that isn't missing values for comparison—and impute (fill in) the missing values using various methods. 
Post Mortem: This is the second program I've written in Python, and the first time I've used pandas, so I'm proud of what I was able to accomplish despite the optimizations to the program that I'm sure could be made. Even though I practically just finished the program, I have a few ideas for how I'd do it differently if I were to do it again from scratch. To start, I believe hot_deck() would benefit from being Cythonized, as well as distance(); the program reaches a CPU bottleneck in that area that contributes to the vast majority of the processing time. For the same reason I would also implement distance() using Python or Numpy arrays rather than pandas Series objects, and avoid most of the nesting for loops in favor of an operation applied to the whole Data Frame at once if possible. By the time I learned about these possible optimizations, the program was in the testing phase and I didn't have time to start over and learn how to do things differently, but I at least know now where to start when implementing a computationally intense algorithm like this in the future.
