# Portfolio
A selection of my classwork and extracurricular projects.

# Project Descriptions

*Sentence Generation Using Ngrams* - Written in Python in February, 2020, for my Intro to Natural Language Processing course at Virginia Commonwealth University. Detailed documentation available in ngram.py. The other files in the folder are the data that were used to train the ngram model; they are Anna Karenina by Leo Tolstoy (1399-0.txt), Crime and Punishment by Fyodor Dostoevsky (2554-0.txt), and War and Peace by Leo Tolstoy (2600-0.txt). Some information, such as each text's table of contents, has been removed to reduce noise.

*eliza.py* - Written in Python in February, 2020, for my Intro to Natural Language Processing course at Virginia Commonwealth University. Detailed documentation available in the file itself.

*Animal Shelter Database* - This was a group project for my Database Theory class in the fall semester of 2019. The project report is something we worked on together, which I've included because it goes into detail about the purpose and design of the database, but the ER Diagram and the .sql files are my own work (not including the population of the tables with data). For the sake of full disclosure, my team members also helped define the requirements and built a front-end application I did not include here. 
    The project required us to upload the database to Google Cloud Platform so that we could attach it to a web application, so to make that possible, the dump file contains only the definition and population of the schemas and their constraints, and can be loaded using MySQL's import tool. After importing the dump file, to see/add the remaining components, including the users, privileges, triggers, and procedures, set "shelter" (without quotes) for use as the default database, then copy the contents of \_Run_after_import_of_dump.sql and paste them into the GCP console. 
 
 *Forming Clusters As Needed* - Written in Java in November, 2019. A detailed project report and requirements document can be found in the project folder, but the general purpose of the project is to sort sentences/"documents" according to their similarity, using the Forming Clusters As Needed unsupervised learning algorithm. This kind of task could be applied to an Information Retrieval setting, like organizing a document or website database to make it searchable, without too much difficulty. The output of my code as it's currently written is optimized for precision more than for recall, so the few clusters that were chosen were groups of sentences that met a high threshold for similarity.

*Efficient_Critical_Definitions.sage* - I wrote this code in Sage in May of 2019, and optimized it over the course of the graph theory research project so that we could efficiently iterate through large sets of graphs and find the ones we cared about.  
    Post Mortem: It was my first time using Python and Sage, as well as gt.sage (a library of graph theory objects and functions that my program assumes is already loaded). If I were to write it again today, I'd look into Cythonizing it to speed it up further, although it served its purpose as-is well enough at the time.  

*CSV Data Imputation* - I wrote this code in Python in September of 2019 for CMSC 435: Intro to Data Science. The purpose of the program is to take three csv files—one missing 5% of the values, one missing 20% of the values, and one that isn't missing values for comparison—and impute (fill in) the missing values using various methods.  
    Post Mortem: This is the second program I've written in Python, and the first time I used pandas, so I'm proud of what I was able to accomplish despite the optimizations to the program that I'm sure could be made. To start, I believe hot_deck() would benefit from being Cythonized, as well as distance(); the program reaches a CPU bottleneck in that area that contributes to the vast majority of the processing time. For the same reason I would also implement distance() using Python or Numpy arrays rather than pandas Series objects, and avoid most of the nesting for loops in favor of an operation applied to the whole Data Frame at once if possible. By the time I learned about these possible optimizations, the program was in the testing phase and I didn't have time to start over and learn how to do things differently, but I at least know now where to start when implementing a computationally intense algorithm like this in the future.
